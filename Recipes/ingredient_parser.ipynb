{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cedcfc-4dde-463c-a98a-1e2e23788e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "# /Users/Jack/Documents/Projects/Whatscooking-/src\n",
    "\n",
    "import pandas as pd \n",
    "import nltk\n",
    "import string\n",
    "import ast\n",
    "import re\n",
    "import unidecode\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "# import config \n",
    "\n",
    "# Weigths and measures are words that will not add value to the model. I got these standard words from \n",
    "# https://en.wikibooks.org/wiki/Cookbook:Units_of_measurement\n",
    "\n",
    "# # We lemmatize the words to reduce them to their smallest form (lemmas). \n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# measures = [lemmatizer.lemmatize(m) for m in measures]\n",
    "# words_to_remove = [lemmatizer.lemmatize(m) for m in words_to_remove]\n",
    "\n",
    "def ingredient_parser(ingreds):\n",
    "    '''\n",
    "    \n",
    "    This function takes in a list (but it is a string as it comes from pandas dataframe) of \n",
    "       ingredients and performs some preprocessing. \n",
    "       For example:\n",
    "\n",
    "       input = '['1 x 1.6kg whole duck', '2 heaped teaspoons Chinese five-spice powder', '1 clementine',\n",
    "                 '6 fresh bay leaves', 'GRAVY', '', '1 bulb of garlic', '2 carrots', '2 red onions', \n",
    "                 '3 tablespoons plain flour', '100 ml Marsala', '1 litre organic chicken stock']'\n",
    "       \n",
    "       output = ['duck', 'chinese five spice powder', 'clementine', 'fresh bay leaf', 'gravy', 'garlic',\n",
    "                 'carrot', 'red onion', 'plain flour', 'marsala', 'organic chicken stock']\n",
    "\n",
    "    '''\n",
    "    measures = ['teaspoon', 't', 'tsp.', 'tablespoon', 'T', 'tbl.', 'tb', 'tbsp.', 'fluid ounce', 'fl oz', 'gill', 'cup', 'c', 'pint', 'p', 'pt', 'fl pt', 'quart', 'q', 'qt', 'fl qt', 'gallon', 'g', 'gal', 'ml', 'milliliter', 'millilitre', 'cc', 'mL', 'l', 'liter', 'litre', 'L', 'dl', 'deciliter', 'decilitre', 'dL', 'bulb', 'level', 'heaped', 'rounded', 'whole', 'pinch', 'medium', 'slice', 'pound', 'lb', '#', 'ounce', 'oz', 'mg', 'milligram', 'milligramme', 'g', 'gram', 'gramme', 'kg', 'kilogram', 'kilogramme', 'x', 'of', 'mm', 'millimetre', 'millimeter', 'cm', 'centimeter', 'centimetre', 'm', 'meter', 'metre', 'inch', 'in', 'milli', 'centi', 'deci', 'hecto', 'kilo']\n",
    "    words_to_remove = ['fresh', 'oil', 'a', 'red', 'bunch', 'and', 'clove', 'or', 'leaf', 'chilli', 'large', 'extra', 'sprig', 'ground', 'handful', 'free', 'small', 'pepper', 'virgin', 'range', 'from', 'dried', 'sustainable', 'black', 'peeled', 'higher', 'welfare', 'seed', 'for', 'finely', 'freshly', 'sea', 'quality', 'white', 'ripe', 'few', 'piece', 'source', 'to', 'organic', 'flat', 'smoked', 'ginger', 'sliced', 'green', 'picked', 'the', 'stick', 'plain', 'plus', 'mixed', 'mint', 'bay', 'basil', 'your', 'cumin', 'optional', 'fennel', 'serve', 'mustard', 'unsalted', 'baby', 'paprika', 'fat', 'ask', 'natural', 'skin', 'roughly', 'into', 'such', 'cut', 'good', 'brown', 'grated', 'trimmed', 'oregano', 'powder', 'yellow', 'dusting', 'knob', 'frozen', 'on', 'deseeded', 'low', 'runny', 'balsamic', 'cooked', 'streaky', 'nutmeg', 'sage', 'rasher', 'zest', 'pin', 'groundnut', 'breadcrumb', 'turmeric', 'halved', 'grating', 'stalk', 'light', 'tinned', 'dry', 'soft', 'rocket', 'bone', 'colour', 'washed', 'skinless', 'leftover', 'splash', 'removed', 'dijon', 'thick', 'big', 'hot', 'drained', 'sized', 'chestnut', 'watercress', 'fishmonger', 'english', 'dill', 'caper', 'raw', 'worcestershire', 'flake', 'cider', 'cayenne', 'tbsp', 'leg', 'pine', 'wild', 'if', 'fine', 'herb', 'almond', 'shoulder', 'cube', 'dressing', 'with', 'chunk', 'spice', 'thumb', 'garam', 'new', 'little', 'punnet', 'peppercorn', 'shelled', 'saffron', 'other''chopped', 'salt', 'olive', 'taste', 'can', 'sauce', 'water', 'diced', 'package', 'italian', 'shredded', 'divided', 'parsley', 'vinegar', 'all', 'purpose', 'crushed', 'juice', 'more', 'coriander', 'bell', 'needed', 'thinly', 'boneless', 'half', 'thyme', 'cubed', 'cinnamon', 'cilantro', 'jar', 'seasoning', 'rosemary', 'extract', 'sweet', 'baking', 'beaten', 'heavy', 'seeded', 'tin', 'vanilla', 'uncooked', 'crumb', 'style', 'thin', 'nut', 'coarsely', 'spring', 'chili', 'cornstarch', 'strip', 'cardamom', 'rinsed', 'honey', 'cherry', 'root', 'quartered', 'head', 'softened', 'container', 'crumbled', 'frying', 'lean', 'cooking', 'roasted', 'warm', 'whipping', 'thawed', 'corn', 'pitted', 'sun', 'kosher', 'bite', 'toasted', 'lasagna', 'split', 'melted', 'degree', 'lengthwise', 'romano', 'packed', 'pod', 'anchovy', 'rom', 'prepared', 'juiced', 'fluid', 'floret', 'room', 'active', 'seasoned', 'mix', 'deveined', 'lightly', 'anise', 'thai', 'size', 'unsweetened', 'torn', 'wedge', 'sour', 'basmati', 'marinara', 'dark', 'temperature', 'garnish', 'bouillon', 'loaf', 'shell', 'reggiano', 'canola', 'parmigiano', 'round', 'canned', 'ghee', 'crust', 'long', 'broken', 'ketchup', 'bulk', 'cleaned', 'condensed', 'sherry', 'provolone', 'cold', 'soda', 'cottage', 'spray', 'tamarind', 'pecorino', 'shortening', 'part', 'bottle', 'sodium', 'cocoa', 'grain', 'french', 'roast', 'stem', 'link', 'firm', 'asafoetida', 'mild', 'dash', 'boiling']\n",
    "    # The ingredient list is now a string so we need to turn it back into a list. We use ast.literal_eval\n",
    "    if isinstance(ingreds, list):\n",
    "        ingredients = ingreds\n",
    "    else:\n",
    "        ingredients = ast.literal_eval(ingreds)\n",
    "    # We first get rid of all the punctuation. We make use of str.maketrans. It takes three input \n",
    "    # arguments 'x', 'y', 'z'. 'x' and 'y' must be equal-length strings and characters in 'x'\n",
    "    # are replaced by characters in 'y'. 'z' is a string (string.punctuation here) where each character\n",
    "    #  in the string is mapped to None. \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ingred_list = []\n",
    "    for i in ingredients:\n",
    "        i.translate(translator)\n",
    "        # We split up with hyphens as well as spaces\n",
    "        items = re.split(' |-', i)\n",
    "        # Get rid of words containing non alphabet letters\n",
    "        items = [word for word in items if word.isalpha()]\n",
    "        # Turn everything to lowercase\n",
    "        items = [word.lower() for word in items]\n",
    "        # remove accents\n",
    "        items = [unidecode.unidecode(word) for word in items] #''.join((c for c in unicodedata.normalize('NFD', items) if unicodedata.category(c) != 'Mn'))\n",
    "        # Lemmatize words so we can compare words to measuring words\n",
    "        items = [lemmatizer.lemmatize(word) for word in items]\n",
    "        # Gets rid of measuring words/phrases, e.g. heaped teaspoon\n",
    "        items = [word for word in items if word not in measures]\n",
    "        # Get rid of common easy words\n",
    "        items = [word for word in items if word not in words_to_remove]\n",
    "        if items:\n",
    "            ingred_list.append(' '.join(items)) \n",
    "    ingred_list = \" \".join(ingred_list)\n",
    "    return ingred_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recipe_df = pd.read_csv('dataset/parsed_data.csv')\n",
    "    recipe_df['NER_parsed'] = recipe_df['NER'].apply(lambda x: ingredient_parser(x))\n",
    "    df = recipe_df[['title', 'NER_parsed', 'NER', 'link']]\n",
    "    df = recipe_df.dropna()\n",
    "\n",
    "    # remove - Allrecipes.com from end of every recipe title \n",
    "    m = df.title.str.endswith('Recipe - Allrecipes.com')\n",
    "    df['title'].loc[m] = df.title.loc[m].str[:-23]        \n",
    "    df.to_csv('dataset/op_2.csv', index=False)\n",
    "\n",
    "\n",
    "    print(df['NER_parsed'].dtypes)\n",
    "    \n",
    "    # print(df.map(type))\n",
    "\n",
    "\n",
    "\n",
    "    # vocabulary = nltk.FreqDist()\n",
    "    # for ingredients in recipe_df['ingredients']:\n",
    "    #     ingredients = ingredients.split()\n",
    "    #     vocabulary.update(ingredients)\n",
    "            \n",
    "    # for word, frequency in vocabulary.most_common(200):\n",
    "    #     print(f'{word};{frequency}')\n",
    "    # fdist = nltk.FreqDist(ingredients)\n",
    "\n",
    "    # common_words = []\n",
    "    # for word, _ in vocabulary.most_common(250):\n",
    "    #     common_words.append(word)\n",
    "    # print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dba268-6be2-4af6-8a5f-c1a468fa40a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
